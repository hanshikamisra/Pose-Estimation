# -*- coding: utf-8 -*-
"""CVPROJECTb22es027.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hcjvb_7uzs9qjfyTppW80QOSc-OdcAW_
"""

# Install required libraries
!pip install torch torchvision matplotlib opencv-python

!pip install datasets

import urllib
urllib.request.urlretrieve("https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz", "mpii_human_pose_v1.tar.gz") #fetching the data from internet.

# Extract the downloaded tar.gz file
!tar -xvzf mpii_human_pose_v1.tar.gz

# List the extracted files and directories in /content/sample_data
!ls /content/sample_data



import urllib
urllib.request.urlretrieve("https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_u12_2.zip", "mpii_human_pose_v1_u12_2.zip") #fetching the data from internet.

!unzip /content/mpii_human_pose_v1_u12_2.zip

import scipy.io

mat_path = "/content/mpii_human_pose_v1_u12_2/mpii_human_pose_v1_u12_1.mat"
mat_data = scipy.io.loadmat(mat_path, struct_as_record=False, squeeze_me=True)

# Get the 'RELEASE' structure which holds the annotations
annotations = mat_data['RELEASE'].annolist

import os
import cv2
import json
import torch
import random
import numpy as np
import scipy.io as sio
from torch.utils.data import Dataset, DataLoader

# Load .mat annotations
mat = sio.loadmat("/content/mpii_human_pose_v1_u12_2/mpii_human_pose_v1_u12_1.mat", struct_as_record=False, squeeze_me=True)
annolist = mat['RELEASE'].annolist
image_base_path = "/content/images"

image_paths = []
keypoints_list = []

for ann in annolist:
    img_name = ann.image.name
    full_img_path = os.path.join(image_base_path, img_name)
    if not os.path.isfile(full_img_path):
        continue

    if hasattr(ann, 'annorect'):
        rects = ann.annorect
        if isinstance(rects, np.ndarray):  # multiple persons
            rects = list(rects)
        else:
            rects = [rects]

        for rect in rects:
            if hasattr(rect, 'annopoints') and hasattr(rect.annopoints, 'point'):
                points = rect.annopoints.point
                if isinstance(points, np.ndarray):
                    points = list(points)
                else:
                    points = [points]

                keypoints = [None] * 16

                for point in points:
                    idx = int(point.id) if hasattr(point, 'id') else None
                    if idx is None or idx >= 16:
                        continue

                    # Handle structured arrays
                    if isinstance(point, np.void):
                        x = int(point['x'])
                        y = int(point['y'])
                        v = point['is_visible'] if 'is_visible' in point.dtype.names else 1
                    else:
                        x = int(point.x)
                        y = int(point.y)
                        v = point.is_visible if hasattr(point, 'is_visible') else 1

                    if isinstance(v, (list, np.ndarray)):
                        visible = int(v[0]) if len(v) > 0 else 1
                    else:
                        visible = int(v)

                    keypoints[idx] = {'position': (x, y), 'visible': visible == 1}

                # Fill missing keypoints
                keypoints = [kp if kp is not None else {'position': (0, 0), 'visible': False} for kp in keypoints]
                image_paths.append(full_img_path)
                keypoints_list.append(keypoints)

# Split into train/val
data = list(zip(image_paths, keypoints_list))
random.shuffle(data)

split_idx = int(0.8 * len(data))
train_data = data[:split_idx]
val_data = data[split_idx:]

train_paths, train_keypoints = zip(*train_data)
val_paths, val_keypoints = zip(*val_data)

# Dataset class
class MPIIDataset(Dataset):
    def __init__(self, image_paths, keypoints, transform=None):
        self.image_paths = image_paths
        self.keypoints = keypoints
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = cv2.imread(self.image_paths[idx])
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        if self.transform:
            image = self.transform(image)

        heatmap = self.generate_heatmap(self.keypoints[idx], image.shape)
        image = torch.tensor(image).permute(2, 0, 1).float() / 255.0
        return image, heatmap

    def generate_heatmap(self, keypoints, image_shape):
        heatmap = np.zeros((16, image_shape[0], image_shape[1]), dtype=np.float32)
        for i, kp in enumerate(keypoints):
            if kp['visible']:
                x, y = kp['position']
                if 0 <= x < image_shape[1] and 0 <= y < image_shape[0]:
                    heatmap[i, y, x] = 1.0
        return torch.tensor(heatmap, dtype=torch.float32)

# Create DataLoaders
train_dataset = MPIIDataset(train_paths, train_keypoints)
val_dataset = MPIIDataset(val_paths, val_keypoints)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

print(f"✅ Loaded {len(train_dataset)} training samples")
print(f"✅ Loaded {len(val_dataset)} validation samples")

import matplotlib.pyplot as plt

# Get one sample from training set
sample_img, sample_heatmap = train_dataset[0]

# Convert back to HWC for plotting
image_np = sample_img.permute(1, 2, 0).numpy()

# Get keypoints from heatmap (argmax)
keypoints = []
for i in range(16):
    y, x = torch.nonzero(sample_heatmap[i], as_tuple=True)
    if len(x) > 0 and len(y) > 0:
        keypoints.append((int(x[0]), int(y[0])))

# Plot
plt.imshow(image_np)
for x, y in keypoints:
    plt.scatter(x, y, c='red', s=20)
plt.title("Sample with Keypoints")
plt.axis('off')
plt.show()

import cv2
import torch
import numpy as np
from torch.utils.data import Dataset
from torchvision import transforms
import math

# Gaussian heatmap helper
def generate_gaussian_heatmap(image_size, keypoints, heatmap_size=(64, 64), sigma=2):
    num_keypoints = len(keypoints)
    heatmaps = np.zeros((num_keypoints, heatmap_size[1], heatmap_size[0]), dtype=np.float32)

    for i, kp in enumerate(keypoints):
        if kp['visible']:
            x, y = kp['position']
            x = x * heatmap_size[0] / image_size[1]
            y = y * heatmap_size[1] / image_size[0]
            xx, yy = int(x), int(y)

            for j in range(heatmap_size[1]):
                for k in range(heatmap_size[0]):
                    d2 = (k - x)**2 + (j - y)**2
                    exponent = d2 / (2 * sigma * sigma)
                    if exponent > 4.6052:  # Cutoff for 0.01
                        continue
                    heatmaps[i, j, k] = math.exp(-exponent)
    return torch.tensor(heatmaps, dtype=torch.float32)

# Image transform
img_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Updated Dataset
class MPIIDatasetHRNet(Dataset):
    def __init__(self, image_paths, keypoints, transform=img_transform):
        self.image_paths = image_paths
        self.keypoints = keypoints
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = cv2.imread(self.image_paths[idx])
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w, _ = image.shape

        heatmap = generate_gaussian_heatmap((h, w), self.keypoints[idx])
        image = self.transform(image)

        return image, heatmap

"""##Mediapipe model"""

pip install mediapipe opencv-python matplotlib

import mediapipe as mp
import cv2
import os
import matplotlib.pyplot as plt

# Setup MediaPipe pose module
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=True, model_complexity=2)
mp_drawing = mp.solutions.drawing_utils

# Path to images
image_dir = "/content/images"
image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir)
                      if f.endswith(('.jpg', '.png', '.jpeg'))])

def visualize_pose(image_path):
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    results = pose.process(image_rgb)

    if results.pose_landmarks:
        annotated = image.copy()
        mp_drawing.draw_landmarks(annotated, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)
        plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))
        plt.title("MediaPipe Pose")
        plt.axis("off")
        plt.show()
    else:
        print("No pose detected in", image_path)

# Try on one image
visualize_pose(image_files[0])

def extract_keypoints(image_path):
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    results = pose.process(image_rgb)
    if not results.pose_landmarks:
        return None

    keypoints = []
    for lm in results.pose_landmarks.landmark:
        keypoints.append({
            'x': lm.x,
            'y': lm.y,
            'z': lm.z,
            'visibility': lm.visibility
        })
    return keypoints

# Example usage
keypoints = extract_keypoints(image_files[0])
print(keypoints[:5])  # Show first 5 keypoints

import os
import random

image_folder = "/content/images"
all_images = sorted([f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png'))])

# Shuffle reproducibly
random.seed(42)
random.shuffle(all_images)

# 80/20 split
split_index = int(0.8 * len(all_images))
train_images = all_images[:split_index]
test_images = all_images[split_index:]

from tqdm import tqdm
import json
import cv2
import mediapipe as mp

def extract_mediapipe_keypoints(image_list, image_dir):
    pose = mp.solutions.pose.Pose(static_image_mode=True)
    keypoints_dict = {}

    for image_name in tqdm(image_list):
        path = os.path.join(image_dir, image_name)
        image = cv2.imread(path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = pose.process(image_rgb)

        keypoints = []
        if results.pose_landmarks:
            for lm in results.pose_landmarks.landmark:
                keypoints.append({
                    'x': lm.x,
                    'y': lm.y,
                    'visibility': lm.visibility
                })

        keypoints_dict[image_name] = keypoints
    return keypoints_dict

train_kps = extract_mediapipe_keypoints(train_images, image_folder)
test_kps = extract_mediapipe_keypoints(test_images, image_folder)

with open("/content/mediapipe_train_keypoints.json", "w") as f:
    json.dump(train_kps, f)

with open("/content/mediapipe_test_keypoints.json", "w") as f:
    json.dump(test_kps, f)

import os
import json
import cv2
import matplotlib.pyplot as plt

# Load keypoints
with open("/content/mediapipe_test_keypoints.json", "r") as f:
    mediapipe_kps = json.load(f)

# Path to test images
image_folder = "/content/images"
test_images = list(mediapipe_kps.keys())

POSE_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 7),
    (0, 4), (4, 5), (5, 6), (6, 8),
    (9, 10), (11, 12), (11, 13), (13, 15),
    (12, 14), (14, 16)
]

def draw_keypoints_and_skeleton(image_path, keypoints, connections=POSE_CONNECTIONS):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    h, w, _ = image.shape

    # Draw keypoints
    for kp in keypoints:
        x = int(kp['x'] * w)
        y = int(kp['y'] * h)
        cv2.circle(image, (x, y), 3, (255, 0, 0), -1)

    # Draw lines between connected keypoints
    for start, end in connections:
        if start < len(keypoints) and end < len(keypoints):
            x1 = int(keypoints[start]['x'] * w)
            y1 = int(keypoints[start]['y'] * h)
            x2 = int(keypoints[end]['x'] * w)
            y2 = int(keypoints[end]['y'] * h)
            cv2.line(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

    plt.figure(figsize=(6, 6))
    plt.imshow(image)
    plt.axis('off')
    plt.show()

for img_name in test_images[:20]:  # Show first 5 test images
    img_path = os.path.join(image_folder, img_name)
    keypoints = mediapipe_kps[img_name]
    draw_keypoints_and_skeleton(img_path, keypoints)



"""## Evaluation"""

# Load .mat annotations
mat = sio.loadmat("/content/mpii_human_pose_v1_u12_2/mpii_human_pose_v1_u12_1.mat", struct_as_record=False, squeeze_me=True)
annolist = mat['RELEASE'].annolist
image_base_path = "/content/images"

data = []  # List to store (image_path, keypoints, head_bbox)

for ann in annolist:
    img_name = ann.image.name
    full_img_path = os.path.join(image_base_path, img_name)
    if not os.path.isfile(full_img_path):
        continue

    if hasattr(ann, 'annorect'):
        rects = ann.annorect
        if isinstance(rects, np.ndarray):  # multiple persons
            rects = list(rects)
        else:
            rects = [rects]

        for rect in rects:
            keypoints = [None] * 16
            head_bbox = None

            # Extract keypoints
            if hasattr(rect, 'annopoints') and hasattr(rect.annopoints, 'point'):
                points = rect.annopoints.point
                if isinstance(points, np.ndarray):
                    points = list(points)
                else:
                    points = [points]

                for point in points:
                    idx = int(point.id) if hasattr(point, 'id') else None
                    if idx is None or idx >= 16:
                        continue

                    if isinstance(point, np.void):
                        x = int(point['x'])
                        y = int(point['y'])
                        v = point['is_visible'] if 'is_visible' in point.dtype.names else 1
                    else:
                        x = int(point.x)
                        y = int(point.y)
                        v = point.is_visible if hasattr(point, 'is_visible') else 1

                    if isinstance(v, (list, np.ndarray)):
                        visible = int(v[0]) if len(v) > 0 else 1
                    else:
                        visible = int(v)

                    keypoints[idx] = {'position': (x, y), 'visible': visible == 1}

            # Extract head bounding box
            if all(hasattr(rect, attr) for attr in ['x1', 'y1', 'x2', 'y2']):
                head_bbox = {
                    'x1': float(rect.x1),
                    'y1': float(rect.y1),
                    'x2': float(rect.x2),
                    'y2': float(rect.y2)
                }

            # Fill missing keypoints
            keypoints = [kp if kp is not None else {'position': (0, 0), 'visible': False} for kp in keypoints]
            data.append((full_img_path, keypoints, head_bbox))

# Split into train/val
random.shuffle(data)
split_idx = int(0.8 * len(data))
train_data = data[:split_idx]
val_data = data[split_idx:]

train_paths, train_keypoints = zip(*[(img_path, kps) for img_path, kps, _ in train_data])
val_paths, val_keypoints = zip(*[(img_path, kps) for img_path, kps, _ in val_data])

"""## EVALUATION OF MEDIAPIPELINE"""

import json
import cv2
import numpy as np
from collections import defaultdict

# Load MediaPipe predictions
with open("/content/mediapipe_test_keypoints.json", "r") as f:
    mediapipe_kps = json.load(f)

# Filter val_data to single-person images
image_to_persons = defaultdict(list)
for image_path, keypoints, head_bbox in val_data:
    image_to_persons[image_path].append((keypoints, head_bbox))

val_data_single = [item for item in val_data if len(image_to_persons[item[0]]) == 1]

# Define mapping from MPII indices to MediaPipe indices
mpii_to_mediapipe = {
    0: 28,  # r ankle
    1: 26,  # r knee
    2: 24,  # r hip
    3: 23,  # l hip
    4: 25,  # l knee
    5: 27,  # l ankle
    10: 16, # r wrist
    11: 14, # r elbow
    12: 12, # r shoulder
    13: 11, # l shoulder
    14: 13, # l elbow
    15: 15  # l wrist
}

def get_predicted_keypoint(pred_keypoints, mpii_idx, w, h):
    """Convert MediaPipe keypoint to pixel coordinates based on MPII index."""
    if mpii_idx in mpii_to_mediapipe:
        mp_idx = mpii_to_mediapipe[mpii_idx]
        if mp_idx < len(pred_keypoints):
            kp = pred_keypoints[mp_idx]
            return (kp['x'] * w, kp['y'] * h)
        return None
    elif mpii_idx == 6:  # pelvis
        left_hip = pred_keypoints[23] if 23 < len(pred_keypoints) else None
        right_hip = pred_keypoints[24] if 24 < len(pred_keypoints) else None
        if left_hip and right_hip:
            x = (left_hip['x'] + right_hip['x']) / 2 * w
            y = (left_hip['y'] + right_hip['y']) / 2 * h
            return (x, y)
        return None
    elif mpii_idx == 7:  # thorax
        left_shoulder = pred_keypoints[11] if 11 < len(pred_keypoints) else None
        right_shoulder = pred_keypoints[12] if 12 < len(pred_keypoints) else None
        if left_shoulder and right_shoulder:
            x = (left_shoulder['x'] + right_shoulder['x']) / 2 * w
            y = (left_shoulder['y'] + right_shoulder['y']) / 2 * h
            return (x, y)
        return None
    elif mpii_idx in [8, 9]:  # upper neck, head top (approximate with nose)
        if 0 < len(pred_keypoints):
            kp = pred_keypoints[0]  # nose
            return (kp['x'] * w, kp['y'] * h)
        return None
    return None

# Evaluation loop
total_visible = 0
correct = 0

for image_path, gt_keypoints, head_bbox in val_data_single:
    image_name = os.path.basename(image_path)
    if image_name not in mediapipe_kps:
        continue

    pred_keypoints = mediapipe_kps[image_name]
    if not pred_keypoints:
        continue

    # Load image to get dimensions
    image = cv2.imread(image_path)
    if image is None:
        continue
    h, w, _ = image.shape

    # Compute head size (diagonal of head bounding box)
    if head_bbox is None:
        continue
    head_size = np.sqrt((head_bbox['x2'] - head_bbox['x1'])**2 + (head_bbox['y2'] - head_bbox['y1'])**2)
    threshold = 0.5 * head_size

    # Compare keypoints
    for mpii_idx in range(16):
        gt_kp = gt_keypoints[mpii_idx]
        if gt_kp is None or not gt_kp['visible']:
            continue

        gt_position = gt_kp['position']
        pred_position = get_predicted_keypoint(pred_keypoints, mpii_idx, w, h)
        if pred_position is None:
            continue

        distance = np.linalg.norm(np.array(gt_position) - np.array(pred_position))
        if distance <= threshold:
            correct += 1
        total_visible += 1

# Compute and display PCKh@0.5
if total_visible > 0:
    pckh = (correct / total_visible) * 100
    print(f"PCKh@0.5: {pckh:.2f}% (Correct: {correct}, Total Visible: {total_visible})")
else:
    print("No visible keypoints available for evaluation.")

import numpy as np
import cv2
import os

def get_predicted_keypoint(pred_keypoints, mpii_idx, width, height):
    # Placeholder: Replace with actual mapping logic from mpii_to_mediapipe
    # Returns [x, y] in pixel coordinates or None if unavailable
    if mpii_idx < len(pred_keypoints):
        pred = pred_keypoints[mpii_idx]
        return [pred['x'] * width, pred['y'] * height] if pred else None
    return None

def compute_mpjpe(val_data_single, mediapipe_kps, mpii_to_mediapipe):
    """
    Compute Mean Per Joint Position Error (MPJPE) in pixels.

    Args:
        val_data_single: List of (image_path, gt_keypoints, head_bbox) tuples.
        mediapipe_kps: Dict mapping image names to predicted keypoints.
        mpii_to_mediapipe: Mapping from MPII indices to MediaPipe indices.
    """
    total_error = 0
    total_visible = 0

    for image_path, gt_keypoints, _ in val_data_single:
        image_name = os.path.basename(image_path)
        if image_name not in mediapipe_kps:
            continue

        pred_keypoints = mediapipe_kps[image_name]
        if not pred_keypoints:
            continue

        image = cv2.imread(image_path)
        if image is None:
            continue
        h, w, _ = image.shape

        for mpii_idx in range(16):  # MPII has 16 keypoints
            gt_kp = gt_keypoints[mpii_idx]
            if gt_kp is None or not gt_kp['visible']:
                continue

            gt_position = gt_kp['position']
            pred_position = get_predicted_keypoint(pred_keypoints, mpii_idx, w, h)
            if pred_position is None:
                continue

            distance = np.linalg.norm(np.array(gt_position) - np.array(pred_position))
            total_error += distance
            total_visible += 1

    if total_visible > 0:
        mpjpe = total_error / total_visible
        print(f"MPJPE: {mpjpe:.2f} pixels")
    else:
        print("No visible keypoints available for MPJPE calculation.")



def compute_precision_recall_f1(val_data_single, mediapipe_kps, mpii_to_mediapipe, threshold=50):
    """
    Compute Precision, Recall, and F1-Score for keypoint detection.

    Args:
        val_data_single: List of (image_path, gt_keypoints, head_bbox) tuples.
        mediapipe_kps: Dict mapping image names to predicted keypoints.
        mpii_to_mediapipe: Mapping from MPII indices to MediaPipe indices.
        threshold: Distance threshold in pixels (default: 50).
    """
    tp = 0  # True positives
    fp = 0  # False positives
    fn = 0  # False negatives

    for image_path, gt_keypoints, _ in val_data_single:
        image_name = os.path.basename(image_path)
        if image_name not in mediapipe_kps:
            continue

        pred_keypoints = mediapipe_kps[image_name]
        if not pred_keypoints:
            continue

        image = cv2.imread(image_path)
        if image is None:
            continue
        h, w, _ = image.shape

        for mpii_idx in range(16):
            gt_kp = gt_keypoints[mpii_idx]
            if gt_kp is None or not gt_kp['visible']:
                continue

            gt_position = gt_kp['position']
            pred_position = get_predicted_keypoint(pred_keypoints, mpii_idx, w, h)

            if pred_position is not None:
                distance = np.linalg.norm(np.array(gt_position) - np.array(pred_position))
                if distance <= threshold:
                    tp += 1
                else:
                    fp += 1
            else:
                fn += 1

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}")

def compute_oks(val_data_single, mediapipe_kps, mpii_to_mediapipe, sigma=0.1):
    """
    Compute Object Keypoint Similarity (OKS).

    Args:
        val_data_single: List of (image_path, gt_keypoints, head_bbox) tuples.
        mediapipe_kps: Dict mapping image names to predicted keypoints.
        mpii_to_mediapipe: Mapping from MPII indices to MediaPipe indices.
        sigma: Standard deviation for OKS (default: 0.1).
    """
    total_oks = 0
    total_keypoints = 0

    for image_path, gt_keypoints, head_bbox in val_data_single:
        image_name = os.path.basename(image_path)
        if image_name not in mediapipe_kps:
            continue

        pred_keypoints = mediapipe_kps[image_name]
        if not pred_keypoints:
            continue

        image = cv2.imread(image_path)
        if image is None:
            continue
        h, w, _ = image.shape

        if head_bbox is None:
            continue
        head_size = np.sqrt((head_bbox['x2'] - head_bbox['x1'])**2 +
                            (head_bbox['y2'] - head_bbox['y1'])**2)
        scale = head_size  # Scale factor based on head size

        oks_sum = 0
        num_visible = 0

        for mpii_idx in range(16):
            gt_kp = gt_keypoints[mpii_idx]
            if gt_kp is None or not gt_kp['visible']:
                continue

            gt_position = gt_kp['position']
            pred_position = get_predicted_keypoint(pred_keypoints, mpii_idx, w, h)
            if pred_position is None:
                continue

            distance = np.linalg.norm(np.array(gt_position) - np.array(pred_position))
            oks = np.exp(-(distance**2) / (2 * (scale * sigma)**2))
            oks_sum += oks
            num_visible += 1

        if num_visible > 0:
            total_oks += oks_sum / num_visible
            total_keypoints += 1

    if total_keypoints > 0:
        average_oks = total_oks / total_keypoints
        print(f"Average OKS: {average_oks:.4f}")
    else:
        print("No visible keypoints available for OKS calculation.")

# Assuming val_data_single, mediapipe_kps, and mpii_to_mediapipe are already defined
print("## EVALUATION OF MEDIAPIPELINE")
# ... existing PCKh evaluation code ...

compute_mpjpe(val_data_single, mediapipe_kps, mpii_to_mediapipe)

compute_oks(val_data_single, mediapipe_kps, mpii_to_mediapipe, sigma=0.1)
compute_precision_recall_f1(val_data_single, mediapipe_kps, mpii_to_mediapipe, threshold=50)





"""
## Heat Map Visualisation"""

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import json
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# --- Step 1: Heatmap Generator ---
def generate_heatmap(image_shape, keypoints, sigma=4):
    h, w = image_shape
    num_kp = len(keypoints)
    heatmap = np.zeros((num_kp, h, w), dtype=np.float32)

    for i, kp in enumerate(keypoints):
        x = int(kp['x'] * w)
        y = int(kp['y'] * h)
        visible = kp.get('visible', 1)
        if visible == 0 or not (0 <= x < w and 0 <= y < h):
            continue
        cv2.circle(heatmap[i], (x, y), sigma, 1, -1)
        heatmap[i] = cv2.GaussianBlur(heatmap[i], (0, 0), sigma)
        if heatmap[i].max() > 0:
            heatmap[i] /= heatmap[i].max()

    return heatmap

# --- Step 2: Dataset Class (No labels) ---
class PoseHeatmapDataset(Dataset):
    def __init__(self, keypoints_dict, image_shape):
        self.keypoints_dict = keypoints_dict
        self.image_shape = image_shape
        self.image_names = list(keypoints_dict.keys())

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, idx):
        img_name = self.image_names[idx]
        keypoints = self.keypoints_dict[img_name]
        heatmap = generate_heatmap(self.image_shape, keypoints)

        if heatmap.shape[0] == 0:
            heatmap = np.zeros((33, *self.image_shape), dtype=np.float32)

        return torch.tensor(heatmap, dtype=torch.float32), img_name

# --- Step 3: CNN Feature Extractor ---
class PoseFeatureExtractor(nn.Module):
    def __init__(self, num_kp=33):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(num_kp, 32, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(64 * 56 * 56, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )

    def forward(self, x):
        return self.net(x)

# --- Step 4: Load Dataset ---
with open("/content/mediapipe_test_keypoints.json", 'r') as f:
    keypoints_dict = json.load(f)

image_shape = (224, 224)
test_dataset = PoseHeatmapDataset(keypoints_dict, image_shape)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

model = PoseFeatureExtractor(num_kp=33)
model.eval()

# --- Step 5: Extract Features ---
embeddings = []
image_names = []

for heatmaps, img_name in test_loader:
    if heatmaps.shape[1] != 33:
        continue
    with torch.no_grad():
        features = model(heatmaps)
        embeddings.append(features.squeeze(0).numpy().astype(np.float64))
        image_names.append(img_name[0])

# --- Step 6: Cluster and Visualize ---
embeddings = np.array(embeddings)
kmeans = KMeans(n_clusters=5, random_state=42)
labels = kmeans.fit_predict(embeddings)

# Reduce to 2D for plotting
pca = PCA(n_components=2)
reduced = pca.fit_transform(embeddings)

# Plot
plt.figure(figsize=(8, 6))
for i in range(5):
    pts = reduced[labels == i]
    plt.scatter(pts[:, 0], pts[:, 1], label=f"Cluster {i}")
plt.legend()
plt.title("Pose Embeddings Clusters")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.grid(True)
plt.show()

# Optionally show a few heatmaps from each cluster
def visualize_pose_prediction(heatmap_tensor, img_name, cluster_id):
    summed = heatmap_tensor.sum(dim=0).numpy()
    plt.imshow(summed, cmap='hot')
    plt.title(f"Cluster {cluster_id}: {img_name}")
    plt.axis('off')
    plt.show()

shown = [0]*5
for heatmaps, img_name in test_loader:
    if heatmaps.shape[1] != 33:
        continue
    with torch.no_grad():
        features = model(heatmaps).squeeze(0).numpy().astype(np.float64)
        cluster_id = kmeans.predict([features])[0]
    if shown[cluster_id] < 3:
        visualize_pose_prediction(heatmaps[0], img_name[0], cluster_id)
        shown[cluster_id] += 1
    if sum(shown) >= 15:
        break









"""## EVALUATION OF MEDIAPIPELINE"""



"""## HRNET Based Model"""

criterion = torch.nn.MSELoss()



"""Pre TRained model"""









