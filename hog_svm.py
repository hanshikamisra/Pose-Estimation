# -*- coding: utf-8 -*-
"""HoG_Svm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TA1Px2JgD8B9kFeZOCQYkITAxiChTKNN

#### 1. Dataset Loading and Exploration
"""

!pip install scikit-image opencv-python scikit-learn tqdm

"""##### Load the .mat annotation file"""

import scipy.io
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load the annotations
mat = scipy.io.loadmat('mpii_human_pose_v1_u12_1.mat')  # or whatever your file is named

annots = mat['annot'][0]  # adjust key if different
img_dir = 'images'  # adjust if needed

pos_patches = []

# Assume head is joint index 9 (from your screenshot: joint_9)
HEAD_IDX = 9
PATCH_SIZE = 40
HALF = PATCH_SIZE // 2

for entry in annots:
    img_name = entry['image_name'][0]
    joints = entry['joint_self'][0, 0]  # shape (2, 16)

    head_x, head_y = joints[0, HEAD_IDX], joints[1, HEAD_IDX]

    # Read image
    img_path = os.path.join(img_dir, img_name)
    img = cv2.imread(img_path)
    if img is None:
        continue

    h, w = img.shape[:2]
    x1, y1 = int(head_x) - HALF, int(head_y) - HALF
    x2, y2 = int(head_x) + HALF, int(head_y) + HALF

    if x1 < 0 or y1 < 0 or x2 > w or y2 > h:
        continue

    patch = img[y1:y2, x1:x2]
    gray_patch = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)
    pos_patches.append(gray_patch)

print("Extracted", len(pos_patches), "positive patches")

import scipy.io

# Load the annotation file
mat_file = "mpii_human_pose_v1_u12_1.mat"  # Make sure the path is correct!
data = scipy.io.loadmat(mat_file)

# Check what keys are inside
print("Keys in MAT file:", data.keys())

# Check the structure inside RELEASE
release_data = data['RELEASE']
print("Type of RELEASE:", type(release_data))
print("Shape of RELEASE:", release_data.shape)
print("Contents inside RELEASE:", release_data.dtype)

# Extract annolist from RELEASE
annolist = release_data['annolist'][0, 0]  # Access the first element properly


print("annolist dtype:", annolist.dtype.names)  # Check what fields it contains

import numpy as np

# Extract the list of images from annolist
images = annolist['image'][0]  # Extract the list of images

# Now iterate and print image names
for i in range(min(5, len(images))):  # Show first 5 images
    image_entry = images[i]  # Each image entry is another array

    # Check if 'name' exists in the structured array and extract it
    if isinstance(image_entry, np.ndarray) and 'name' in image_entry.dtype.names:
        # Extract the actual filename
        print(f"Image {i + 1}: {image_entry['name'][0][0]}")
    else:
        print(f"Image {i + 1}: Unexpected structure - {image_entry}")

# Get annotation data
annotations = annolist['annorect']


# Check the type and shape
print(f"Type of annotations: {type(annotations)}")
print(f"Shape of annotations: {annotations.shape}")

# Print the first annotation entry
print("First annotation sample:")
print(annotations[0])

"""#####  Extract image names and joint positions"""

import scipy.io
import os
import numpy as np

# Load the .mat file
mat_file = 'mpii_human_pose_v1_u12_1.mat'
data = scipy.io.loadmat(mat_file, struct_as_record=False, squeeze_me=True)

release_data = data['RELEASE']
annolist = release_data.annolist

image_base_path = 'mpii_human_pose_v1/images'

image_paths = []
keypoints_list = []

print(f"Length of annolist: {len(annolist)}")

for ann_idx, ann in enumerate(annolist):
    try:
        # Get image name
        img_name = getattr(ann.image, 'name', None)
        if not isinstance(img_name, str):
            continue

        full_img_path = os.path.join(image_base_path, img_name)
        if not os.path.isfile(full_img_path):
            print(f"Image does not exist: {full_img_path}")
            continue

        # Get annotation rectangles
        if not hasattr(ann, 'annorect'):
            continue

        rects = ann.annorect
        if rects is None:
            continue

        if not isinstance(rects, np.ndarray):
            rects = [rects]

        for rect in rects:
            if not hasattr(rect, 'annopoints') or rect.annopoints is None:
                continue

            annopoints = rect.annopoints

            # Skip if annopoints is empty or not valid
            if isinstance(annopoints, np.ndarray):
                if annopoints.size == 0:
                    continue
                annopoints = annopoints.item()

            if not hasattr(annopoints, 'point'):
                continue

            points = annopoints.point
            if points is None:
                continue

            # Handle different point structures
            if isinstance(points, np.ndarray):
                if points.size == 0:  # Empty array
                    continue
                elif points.ndim == 0:  # Single item
                    points = [points.item()]
            elif not isinstance(points, (list, np.ndarray)):
                points = [points]

            keypoints = [None] * 16

            for point in points:
                try:
                    # More robust type handling
                    if hasattr(point, 'id'):
                        if isinstance(point.id, np.ndarray):
                            if point.id.size == 0:
                                continue
                            idx = int(point.id.item()) if point.id.size == 1 else int(point.id[0])
                        else:
                            idx = int(point.id)
                    else:
                        continue

                    if hasattr(point, 'x'):
                        if isinstance(point.x, np.ndarray):
                            if point.x.size == 0:
                                continue
                            x = int(point.x.item()) if point.x.size == 1 else int(point.x[0])
                        else:
                            x = int(point.x)
                    else:
                        continue

                    if hasattr(point, 'y'):
                        if isinstance(point.y, np.ndarray):
                            if point.y.size == 0:
                                continue
                            y = int(point.y.item()) if point.y.size == 1 else int(point.y[0])
                        else:
                            y = int(point.y)
                    else:
                        continue

                    v = 1  # Default visibility
                    if hasattr(point, 'is_visible'):
                        if isinstance(point.is_visible, np.ndarray):
                            if point.is_visible.size == 0:
                                v = 1  # Default to visible if array is empty
                            else:
                                v = int(point.is_visible.item()) if point.is_visible.size == 1 else int(point.is_visible[0])
                        elif isinstance(point.is_visible, (int, float)):
                            v = int(point.is_visible)
                        elif isinstance(point.is_visible, str):
                            # Try to convert string to int if possible
                            try:
                                v = int(point.is_visible)
                            except ValueError:
                                v = 1 if point.is_visible.lower() == 'true' else 0

                    if idx >= 16 or idx < 0:
                        continue

                    keypoints[idx] = {
                        'position': (x, y),
                        'visible': v == 1
                    }
                except Exception as e:
                    print(f"Error reading keypoint: {e} - Type info: id={type(getattr(point, 'id', None))}, "
                          f"x={type(getattr(point, 'x', None))}, y={type(getattr(point, 'y', None))}")
                    continue

            # Fill missing keypoints
            keypoints = [
                kp if kp is not None else {'position': (0, 0), 'visible': False}
                for kp in keypoints
            ]

            image_paths.append(full_img_path)
            keypoints_list.append(keypoints)
    except Exception as e:
        print(f"Error processing annotation {ann_idx}: {e}")
        continue

# Display sample results
print(f"\nTotal annotated persons extracted: {len(image_paths)}")
for i in range(min(5, len(image_paths))):
    print(f"\nImage {i + 1}: {image_paths[i]}")
    for j, kp in enumerate(keypoints_list[i]):
        print(f"  Joint {j}: Position = {kp['position']}, Visible = {kp['visible']}")

# Save the extracted data to avoid reprocessing
import pickle
with open('mpii_processed_data.pkl', 'wb') as f:
    pickle.dump({
        'image_paths': image_paths,
        'keypoints_list': keypoints_list
    }, f)
print("\nData saved to mpii_processed_data.pkl")

import pickle
import numpy as np
import cv2
from skimage.feature import hog
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_curve, average_precision_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os
from tqdm import tqdm
import random
import joblib

# --- CONFIG ---
PATCH_SIZE = 64
JOINT_INDEX = 9
HOG_CELL_SIZE = (8, 8)
HOG_BLOCK_SIZE = (2, 2)
HOG_BINS = 9
RANDOM_SEED = 42

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# --- LOAD DATA ---
with open('mpii_processed_data.pkl', 'rb') as f:
    data = pickle.load(f)

image_paths = data['image_paths']
keypoints_list = data['keypoints_list']
print(f"Loaded {len(image_paths)} annotated persons")


# --- PATCH EXTRACTION ---
def extract_patches(image_paths, keypoints_list, joint_index, patch_size, max_samples=5000):
    positive_patches, negative_patches = [], []
    half_size = patch_size // 2

    indices = random.sample(range(len(image_paths)), min(max_samples, len(image_paths)))

    for idx in tqdm(indices, desc="Extracting patches"):
        img_path = image_paths[idx]
        keypoints = keypoints_list[idx]

        try:
            img = cv2.imread(img_path)
            if img is None:
                continue
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape

            joint = keypoints[joint_index]
            x, y = joint['position']
            visible = joint['visible']

            if not visible or (x, y) == (0, 0):
                continue

            if half_size <= x < w - half_size and half_size <= y < h - half_size:
                pos_patch = gray[y - half_size:y + half_size, x - half_size:x + half_size]
                if pos_patch.shape == (patch_size, patch_size):
                    positive_patches.append(pos_patch)

            for _ in range(2):
                for _ in range(10):  # attempt up to 10 times to find a valid negative
                    nx = random.randint(half_size, w - half_size - 1)
                    ny = random.randint(half_size, h - half_size - 1)
                    if all(np.linalg.norm(np.array([nx, ny]) - np.array(kp['position'])) >= patch_size
                           for kp in keypoints if kp['position'] != (0, 0)):
                        neg_patch = gray[ny - half_size:ny + half_size, nx - half_size:nx + half_size]
                        if neg_patch.shape == (patch_size, patch_size):
                            negative_patches.append(neg_patch)
                        break
        except Exception as e:
            print(f"[ERROR] {img_path}: {e}")

    return np.array(positive_patches), np.array(negative_patches)


# --- FEATURE EXTRACTION ---
def extract_hog_features(patches):
    return np.array([
        hog(patch, orientations=HOG_BINS, pixels_per_cell=HOG_CELL_SIZE,
            cells_per_block=HOG_BLOCK_SIZE, visualize=False, feature_vector=True)
        for patch in tqdm(patches, desc="Extracting HOG features")
    ])


# --- TRAIN & EVALUATE ---
def train_svm(X_train, y_train):
    svm = LinearSVC(random_state=RANDOM_SEED, class_weight='balanced')
    svm.fit(X_train, y_train)
    return svm


def evaluate_model(svm, X_test, y_test):
    y_pred = svm.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))

    cm = confusion_matrix(y_test, y_pred)
    plt.figure()
    plt.title("Confusion Matrix")
    plt.imshow(cm, cmap='Blues')
    plt.colorbar()
    plt.xticks([0, 1], ['Negative', 'Positive'])
    plt.yticks([0, 1], ['Negative', 'Positive'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig("confusion_matrix.png")
    plt.show()

    probs = svm.decision_function(X_test)
    precision, recall, _ = precision_recall_curve(y_test, probs)
    ap_score = average_precision_score(y_test, probs)

    plt.figure()
    plt.plot(recall, precision, label=f"AP = {ap_score:.2f}")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve")
    plt.legend()
    plt.savefig("precision_recall_curve.png")
    plt.show()


# --- MAIN PIPELINE ---
def main():
    print("Extracting patches...")
    pos_patches, neg_patches = extract_patches(image_paths, keypoints_list, JOINT_INDEX, PATCH_SIZE, max_samples=3000)
    print(f"Positive: {len(pos_patches)}, Negative: {len(neg_patches)}")

    print("Extracting HOG features...")
    pos_features = extract_hog_features(pos_patches)
    neg_features = extract_hog_features(neg_patches)

    X = np.vstack([pos_features, neg_features])
    y = np.hstack([np.ones(len(pos_features)), np.zeros(len(neg_features))])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)
    print(f"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

    print("Training SVM...")
    svm = train_svm(X_train, y_train)

    print("Evaluating...")
    evaluate_model(svm, X_test, y_test)

    print("Saving model...")
    joblib.dump(svm, "head_detector_svm.pkl")
    joblib.dump({
        'patch_size': PATCH_SIZE,
        'hog_cell_size': HOG_CELL_SIZE,
        'hog_block_size': HOG_BLOCK_SIZE,
        'hog_bins': HOG_BINS
    }, "model_parameters.pkl")

    print("Done")


if __name__ == "__main__":
    main()

import pickle
import numpy as np
import cv2
from skimage.feature import hog
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import os
from tqdm import tqdm
import random

# Load your processed data
with open('mpii_processed_data.pkl', 'rb') as f:
    data = pickle.load(f)

image_paths = data['image_paths']
keypoints_list = data['keypoints_list']

print(f"Loaded {len(image_paths)} annotated persons")

# Define parameters
PATCH_SIZE = 64  # Size of patch to extract around joints
JOINT_INDEX = 9  # Head joint (based on your earlier code)
HOG_CELL_SIZE = (8, 8)
HOG_BLOCK_SIZE = (2, 2)
HOG_BINS = 9

# Function to extract positive and negative patches
def extract_patches(image_paths, keypoints_list, joint_index, patch_size, max_samples=5000):
    positive_patches = []
    negative_patches = []

    half_size = patch_size // 2

    # Randomly select samples to avoid memory issues
    indices = list(range(len(image_paths)))
    random.shuffle(indices)
    indices = indices[:min(len(indices), max_samples)]

    for idx in tqdm(indices, desc="Extracting patches"):
        img_path = image_paths[idx]
        keypoints = keypoints_list[idx]

        try:
            # Read image
            img = cv2.imread(img_path)
            if img is None:
                continue

            # Convert to grayscale
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape

            # Get joint position
            joint = keypoints[joint_index]
            joint_pos = joint['position']
            joint_visible = joint['visible']

            # Skip if joint is not visible or position is (0, 0) (missing)
            if not joint_visible or joint_pos == (0, 0):
                continue

            x, y = joint_pos

            # Extract positive patch (centered on joint)
            if x >= half_size and y >= half_size and x + half_size < w and y + half_size < h:
                pos_patch = gray[y - half_size:y + half_size, x - half_size:x + half_size]
                if pos_patch.shape == (patch_size, patch_size):
                    positive_patches.append(pos_patch)

            # Generate negative patches (away from any joint)
            for _ in range(2):  # Generate 2 negative samples per positive
                # Find a spot away from all joints
                valid_spot = False
                attempts = 0

                while not valid_spot and attempts < 10:
                    attempts += 1
                    nx = random.randint(half_size, w - half_size - 1)
                    ny = random.randint(half_size, h - half_size - 1)

                    # Check distance from all joints
                    valid_spot = True
                    for kp in keypoints:
                        kp_pos = kp['position']
                        if kp_pos == (0, 0):  # Skip missing joints
                            continue

                        # If too close to any joint, this is not a valid negative sample
                        dist = np.sqrt((nx - kp_pos[0])**2 + (ny - kp_pos[1])**2)
                        if dist < patch_size:
                            valid_spot = False
                            break

                if valid_spot:
                    neg_patch = gray[ny - half_size:ny + half_size, nx - half_size:nx + half_size]
                    if neg_patch.shape == (patch_size, patch_size):
                        negative_patches.append(neg_patch)

        except Exception as e:
            print(f"Error processing image {img_path}: {e}")

    return np.array(positive_patches), np.array(negative_patches)

# Extract patches for training
positive_patches, negative_patches = extract_patches(
    image_paths, keypoints_list,
    joint_index=JOINT_INDEX,
    patch_size=PATCH_SIZE,
    max_samples=3000  # Limit samples to avoid memory issues
)

print(f"Extracted {len(positive_patches)} positive patches and {len(negative_patches)} negative patches")

# Visualize some patches
def visualize_patches(positive_patches, negative_patches, num_samples=5):
    plt.figure(figsize=(12, 6))

    for i in range(num_samples):
        # Positive patches
        plt.subplot(2, num_samples, i + 1)
        plt.imshow(positive_patches[i], cmap='gray')
        plt.title(f"Positive {i+1}")
        plt.axis('off')

        # Negative patches
        plt.subplot(2, num_samples, i + num_samples + 1)
        plt.imshow(negative_patches[i], cmap='gray')
        plt.title(f"Negative {i+1}")
        plt.axis('off')

    plt.tight_layout()
    plt.savefig('sample_patches.png')
    plt.show()

visualize_patches(positive_patches, negative_patches)

# Extract HOG features
def extract_hog_features(patches):
    features = []

    for patch in tqdm(patches, desc="Extracting HOG features"):
        feature = hog(
            patch,
            orientations=HOG_BINS,
            pixels_per_cell=HOG_CELL_SIZE,
            cells_per_block=HOG_BLOCK_SIZE,
            visualize=False,
            feature_vector=True
        )
        features.append(feature)

    return np.array(features)

# Extract features
positive_features = extract_hog_features(positive_patches)
negative_features = extract_hog_features(negative_patches)

# Create labels
positive_labels = np.ones(len(positive_features))
negative_labels = np.zeros(len(negative_features))

# Combine data
X = np.vstack((positive_features, negative_features))
y = np.hstack((positive_labels, negative_labels))

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Testing set: {X_test.shape[0]} samples")

# Train SVM classifier
print("Training SVM classifier...")
svm = LinearSVC(random_state=42)
svm.fit(X_train, y_train)

# Evaluate the classifier
y_pred = svm.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))

# Save the model and data
import joblib
joblib.dump(svm, 'head_detector_svm.pkl')
joblib.dump({
    'patch_size': PATCH_SIZE,
    'hog_cell_size': HOG_CELL_SIZE,
    'hog_block_size': HOG_BLOCK_SIZE,
    'hog_bins': HOG_BINS
}, 'model_parameters.pkl')

# Function to detect joints in a new image
def detect_joint(image_path, svm, patch_size=PATCH_SIZE, stride=16, threshold=0.0):
    # Load image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Could not read image: {image_path}")
        return None, None

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    h, w = gray.shape

    half_size = patch_size // 2
    detections = []
    confidences = []

    # Slide window over image
    for y in range(half_size, h - half_size, stride):
        for x in range(half_size, w - half_size, stride):
            # Extract patch
            patch = gray[y - half_size:y + half_size, x - half_size:x + half_size]

            if patch.shape != (patch_size, patch_size):
                continue

            # Extract HOG features
            features = hog(
                patch,
                orientations=HOG_BINS,
                pixels_per_cell=HOG_CELL_SIZE,
                cells_per_block=HOG_BLOCK_SIZE,
                visualize=False,
                feature_vector=True
            )

            # Predict
            features = features.reshape(1, -1)
            confidence = svm.decision_function(features)[0]

            if confidence > threshold:
                detections.append((x, y))
                confidences.append(confidence)

    return detections, confidences

# Test on a few images
def test_detection(image_paths, svm, num_samples=3):
    plt.figure(figsize=(15, 5 * num_samples))

    indices = random.sample(range(len(image_paths)), min(num_samples, len(image_paths)))

    for i, idx in enumerate(indices):
        img_path = image_paths[idx]
        keypoints = keypoints_list[idx]

        # Load image
        img = cv2.imread(img_path)
        if img is None:
            continue

        # Detect joints
        detections, confidences = detect_joint(img_path, svm, threshold=-0.5)

        # Find the ground truth position
        gt_pos = keypoints[JOINT_INDEX]['position']

        # Plot
        plt.subplot(num_samples, 1, i + 1)
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

        # Plot ground truth
        if gt_pos != (0, 0):
            plt.plot(gt_pos[0], gt_pos[1], 'go', markersize=10, label='Ground Truth')

        # Plot detections
        if detections:
            detections = np.array(detections)
            # Get the detection with maximum confidence
            max_idx = np.argmax(confidences)
            max_detection = detections[max_idx]

            plt.plot(max_detection[0], max_detection[1], 'ro', markersize=10, label='SVM Detection')

            # Calculate error
            if gt_pos != (0, 0):
                error = np.sqrt((gt_pos[0] - max_detection[0])**2 + (gt_pos[1] - max_detection[1])**2)
                plt.title(f"Detection Error: {error:.2f} pixels")

        plt.legend()

    plt.tight_layout()
    plt.savefig('detection_results.png')
    plt.show()

# Test on a few images
test_indices = random.sample(range(len(image_paths)), 3)
test_image_paths = [image_paths[i] for i in test_indices]
test_detection(test_image_paths, svm)

# Evaluate on a larger test set
def evaluate_detector(image_paths, keypoints_list, svm, joint_index=JOINT_INDEX, num_samples=100, error_threshold=30):
    # Select a subset for evaluation
    indices = random.sample(range(len(image_paths)), min(num_samples, len(image_paths)))

    errors = []
    detection_rate = 0

    for idx in tqdm(indices, desc="Evaluating detector"):
        img_path = image_paths[idx]
        keypoints = keypoints_list[idx]

        # Skip if joint is not visible
        if not keypoints[joint_index]['visible'] or keypoints[joint_index]['position'] == (0, 0):
            continue

        # Ground truth position
        gt_pos = keypoints[joint_index]['position']

        # Detect joint
        detections, confidences = detect_joint(img_path, svm, threshold=-0.5)

        if not detections:
            continue

        # Get the detection with maximum confidence
        max_idx = np.argmax(confidences)
        max_detection = detections[max_idx]

        # Calculate error
        error = np.sqrt((gt_pos[0] - max_detection[0])**2 + (gt_pos[1] - max_detection[1])**2)
        errors.append(error)

        # Check if detection is within threshold
        if error < error_threshold:
            detection_rate += 1

    # Calculate statistics
    if errors:
        mean_error = np.mean(errors)
        median_error = np.median(errors)
        pck = detection_rate / len(errors)  # Percentage of Correct Keypoints

        print(f"Evaluation Results (Joint {joint_index}):")
        print(f"Mean Error: {mean_error:.2f} pixels")
        print(f"Median Error: {median_error:.2f} pixels")
        print(f"PCK@{error_threshold}: {pck:.4f}")

        return mean_error, median_error, pck
    else:
        print("No valid detections found.")
        return None, None, None

# Evaluate on a larger test set
evaluate_detector(image_paths, keypoints_list, svm, num_samples=50)



import cv2
import numpy as np
import matplotlib.pyplot as plt
import joblib
from skimage.feature import hog
from sklearn.svm import LinearSVC
from tqdm import tqdm
import os
import random

# Define joint connections for skeleton visualization
# Each tuple represents a connection between two joint indices
SKELETON_CONNECTIONS = [
    (0, 1),  # Right ankle -> Right knee
    (1, 2),  # Right knee -> Right hip
    (2, 6),  # Right hip -> Pelvis
    (6, 3),  # Pelvis -> Left hip
    (3, 4),  # Left hip -> Left knee
    (4, 5),  # Left knee -> Left ankle
    (6, 7),  # Pelvis -> Thorax
    (7, 8),  # Thorax -> Neck
    (8, 9),  # Neck -> Head
    (7, 12), # Thorax -> Right shoulder
    (12, 11), # Right shoulder -> Right elbow
    (11, 10), # Right elbow -> Right wrist
    (7, 13), # Thorax -> Left shoulder
    (13, 14), # Left shoulder -> Left elbow
    (14, 15)  # Left elbow -> Left wrist
]

# Define joint names for reference
JOINT_NAMES = [
    'Right ankle', 'Right knee', 'Right hip',
    'Left hip', 'Left knee', 'Left ankle',
    'Pelvis', 'Thorax', 'Neck', 'Head',
    'Right wrist', 'Right elbow', 'Right shoulder',
    'Left shoulder', 'Left elbow', 'Left wrist'
]

# Joint colors for visualization
JOINT_COLORS = [
    (255, 0, 0),    # Right ankle (Red)
    (255, 85, 0),   # Right knee (Orange-Red)
    (255, 170, 0),  # Right hip (Orange)
    (0, 255, 0),    # Left hip (Green)
    (0, 255, 85),   # Left knee (Light Green)
    (0, 255, 170),  # Left ankle (Teal)
    (255, 255, 0),  # Pelvis (Yellow)
    (170, 255, 0),  # Thorax (Yellow-Green)
    (85, 255, 0),   # Neck (Lime Green)
    (0, 0, 255),    # Head (Blue)
    (255, 0, 255),  # Right wrist (Magenta)
    (170, 0, 255),  # Right elbow (Purple)
    (85, 0, 255),   # Right shoulder (Violet)
    (0, 255, 255),  # Left shoulder (Cyan)
    (0, 170, 255),  # Left elbow (Light Blue)
    (0, 85, 255)    # Left wrist (Sky Blue)
]

# Connection colors
LINE_COLOR = (0, 255, 255)  # Cyan for skeleton connections
GT_LINE_COLOR = (255, 255, 0)  # Yellow for ground truth connections

def detect_joint(image_path, svm, joint_index, patch_size=64, stride=16, threshold=0.0,
                scale_factor=1.0, multi_scale=True):
    """
    Detect a specific joint in an image using sliding window and SVM classifier.
    Returns the positions of all detections and their confidence scores.
    """
    # Load image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Could not read image: {image_path}")
        return [], []

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Multi-scale detection
    scales = [0.7, 1.0, 1.3] if multi_scale else [1.0]
    all_detections = []
    all_confidences = []

    # HOG parameters (should match training parameters)
    hog_cell_size = (8, 8)
    hog_block_size = (2, 2)
    hog_bins = 9

    for scale in scales:
        # Resize image based on scale
        if scale != 1.0:
            h, w = gray.shape
            resized = cv2.resize(gray, (int(w * scale), int(h * scale)))
        else:
            resized = gray

        h, w = resized.shape
        half_size = patch_size // 2

        # Sliding window
        for y in range(half_size, h - half_size, stride):
            for x in range(half_size, w - half_size, stride):
                # Extract patch
                patch = resized[y - half_size:y + half_size, x - half_size:x + half_size]

                if patch.shape != (patch_size, patch_size):
                    continue

                # Extract HOG features
                features = hog(
                    patch,
                    orientations=hog_bins,
                    pixels_per_cell=hog_cell_size,
                    cells_per_block=hog_block_size,
                    visualize=False,
                    feature_vector=True
                )

                # Predict
                features = features.reshape(1, -1)
                confidence = svm.decision_function(features)[0]

                if confidence > threshold:
                    # Convert coordinates back to original scale
                    orig_x = int(x / scale)
                    orig_y = int(y / scale)
                    all_detections.append((orig_x, orig_y))
                    all_confidences.append(confidence)

    return all_detections, all_confidences

def non_maximum_suppression(detections, confidences, threshold=20):
    """
    Apply non-maximum suppression to remove duplicate detections.
    """
    if not detections:
        return [], []

    # Convert to numpy arrays
    detections = np.array(detections)
    confidences = np.array(confidences)

    # Sort by confidence (descending)
    idx = np.argsort(-confidences)
    detections = detections[idx]
    confidences = confidences[idx]

    keep = []

    while len(detections) > 0:
        # Add the detection with highest confidence
        keep.append(0)

        if len(detections) == 1:
            break

        # Compute distances to all other detections
        current = detections[0]
        others = detections[1:]

        distances = np.sqrt(np.sum((others - current) ** 2, axis=1))

        # Remove detections that are too close
        mask = distances > threshold
        detections = others[mask]
        confidences = confidences[1:][mask]

    return detections[keep].tolist(), confidences[keep].tolist()

def detect_all_joints(image_path, joint_detectors, threshold=-0.5):
    """
    Detect all joints in an image and return their positions.
    """
    joint_positions = {}

    for joint_idx, detector in joint_detectors.items():
        detections, confidences = detect_joint(
            image_path, detector, joint_idx, threshold=threshold
        )

        # Apply non-maximum suppression
        detections, confidences = non_maximum_suppression(detections, confidences)

        if detections:
            # Get the detection with highest confidence
            max_idx = np.argmax(confidences)
            joint_positions[joint_idx] = detections[max_idx]

    return joint_positions

def train_joint_detector(joint_idx, image_paths, keypoints_list, n_samples=3000):
    """
    Train an SVM detector for a specific joint.
    """
    print(f"Training detector for joint {joint_idx} ({JOINT_NAMES[joint_idx]})...")

    PATCH_SIZE = 64
    HOG_CELL_SIZE = (8, 8)
    HOG_BLOCK_SIZE = (2, 2)
    HOG_BINS = 9

    # Extract patches
    pos_patches, neg_patches = extract_patches(
        image_paths, keypoints_list, joint_idx, PATCH_SIZE, max_samples=n_samples
    )

    if len(pos_patches) < 50 or len(neg_patches) < 50:
        print(f"Not enough data for joint {joint_idx}. Positive: {len(pos_patches)}, Negative: {len(neg_patches)}")
        return None

    print(f"Extracted {len(pos_patches)} positive and {len(neg_patches)} negative patches")

    # Extract HOG features
    pos_features = extract_hog_features(pos_patches)
    neg_features = extract_hog_features(neg_patches)

    # Create dataset
    X = np.vstack([pos_features, neg_features])
    y = np.hstack([np.ones(len(pos_features)), np.zeros(len(neg_features))])

    # Train SVM
    svm = LinearSVC(random_state=42, class_weight='balanced')
    svm.fit(X, y)

    return svm

def extract_patches(image_paths, keypoints_list, joint_index, patch_size, max_samples=5000):
    """
    Extract positive and negative patches for training.
    """
    positive_patches = []
    negative_patches = []

    half_size = patch_size // 2

    # Randomly select samples to avoid memory issues
    indices = random.sample(range(len(image_paths)), min(max_samples, len(image_paths)))

    for idx in tqdm(indices, desc="Extracting patches"):
        try:
            img_path = image_paths[idx]
            keypoints = keypoints_list[idx]

            # Read image
            img = cv2.imread(img_path)
            if img is None:
                continue

            # Convert to grayscale
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape

            # Get joint position
            joint = keypoints[joint_index]
            joint_pos = joint['position']
            joint_visible = joint['visible']

            # Skip if joint is not visible or position is (0, 0) (missing)
            if not joint_visible or joint_pos == (0, 0):
                continue

            x, y = joint_pos

            # Extract positive patch (centered on joint)
            if half_size <= x < w - half_size and half_size <= y < h - half_size:
                pos_patch = gray[y - half_size:y + half_size, x - half_size:x + half_size]
                if pos_patch.shape == (patch_size, patch_size):
                    positive_patches.append(pos_patch)

            # Extract negative patches (away from any joint)
            for _ in range(3):  # Generate 3 negative samples per positive
                for _ in range(10):  # Try up to 10 times to find a valid spot
                    nx = random.randint(half_size, w - half_size - 1)
                    ny = random.randint(half_size, h - half_size - 1)

                    # Check if far enough from all joints
                    valid = True
                    for kp in keypoints:
                        kp_pos = kp['position']
                        if kp_pos == (0, 0):  # Skip missing joints
                            continue

                        dist = np.sqrt((nx - kp_pos[0])**2 + (ny - kp_pos[1])**2)
                        if dist < patch_size:
                            valid = False
                            break

                    if valid:
                        neg_patch = gray[ny - half_size:ny + half_size, nx - half_size:nx + half_size]
                        if neg_patch.shape == (patch_size, patch_size):
                            negative_patches.append(neg_patch)
                            break
        except Exception as e:
            print(f"Error extracting patches from {img_path}: {e}")

    return np.array(positive_patches), np.array(negative_patches)

def extract_hog_features(patches):
    """
    Extract HOG features from image patches.
    """
    hog_cell_size = (8, 8)
    hog_block_size = (2, 2)
    hog_bins = 9

    features = []
    for patch in tqdm(patches, desc="Extracting HOG features"):
        feature = hog(
            patch,
            orientations=hog_bins,
            pixels_per_cell=hog_cell_size,
            cells_per_block=hog_block_size,
            visualize=False,
            feature_vector=True
        )
        features.append(feature)

    return np.array(features)

def visualize_skeleton(image_path, joint_positions, ground_truth=None):
    """
    Visualize skeleton by drawing lines between detected joints.
    """
    # Load image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Could not read image: {image_path}")
        return None

    # Create a copy for visualization
    vis_img = img.copy()

    # Draw detected skeleton
    for connection in SKELETON_CONNECTIONS:
        joint1, joint2 = connection
        if joint1 in joint_positions and joint2 in joint_positions:
            pt1 = tuple(map(int, joint_positions[joint1]))
            pt2 = tuple(map(int, joint_positions[joint2]))
            cv2.line(vis_img, pt1, pt2, LINE_COLOR, 2)

    # Draw detected joints
    for joint_idx, pos in joint_positions.items():
        x, y = map(int, pos)
        cv2.circle(vis_img, (x, y), 5, JOINT_COLORS[joint_idx], -1)
        cv2.putText(vis_img, str(joint_idx), (x+5, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    # Draw ground truth if provided
    if ground_truth is not None:
        for connection in SKELETON_CONNECTIONS:
            joint1, joint2 = connection
            if (joint1 < len(ground_truth) and joint2 < len(ground_truth) and
                ground_truth[joint1]['visible'] and ground_truth[joint2]['visible'] and
                ground_truth[joint1]['position'] != (0, 0) and ground_truth[joint2]['position'] != (0, 0)):
                pt1 = tuple(map(int, ground_truth[joint1]['position']))
                pt2 = tuple(map(int, ground_truth[joint2]['position']))
                cv2.line(vis_img, pt1, pt2, GT_LINE_COLOR, 1, cv2.LINE_AA)

        for joint_idx, joint in enumerate(ground_truth):
            if joint['visible'] and joint['position'] != (0, 0):
                x, y = map(int, joint['position'])
                cv2.circle(vis_img, (x, y), 3, (0, 0, 255), -1)

    return vis_img

def apply_anatomical_constraints(joint_positions):
    """
    Apply anatomical constraints to filter out implausible joint configurations.
    """
    if not joint_positions:
        return joint_positions

    # Example constraint: The head should be above the thorax
    if 9 in joint_positions and 7 in joint_positions:
        head_y = joint_positions[9][1]
        thorax_y = joint_positions[7][1]
        if head_y > thorax_y:  # In image coordinates, smaller y is higher
            # This is implausible, adjust or remove
            # For simplicity, we'll just adjust the head position
            joint_positions[9] = (joint_positions[9][0], thorax_y - 30)

    # Example constraint: Left and right sides should be symmetric
    # ... more constraints could be added here

    return joint_positions

def main():
    # Path to your processed data
    with open('mpii_processed_data.pkl', 'rb') as f:
        data = pickle.load(f)

    image_paths = data['image_paths']
    keypoints_list = data['keypoints_list']

    # Check if we have pre-trained detectors, otherwise train them
    try:
        joint_detectors = joblib.load('joint_detectors.pkl')
        print("Loaded pre-trained joint detectors")
    except:
        print("Training joint detectors...")
        joint_detectors = {}

        # You can limit the joints to train to save time
        # Here we're training detectors for all 16 joints
        for joint_idx in range(16):
            detector = train_joint_detector(joint_idx, image_paths, keypoints_list)
            if detector is not None:
                joint_detectors[joint_idx] = detector
                # Save after each detector to resume if interrupted
                joblib.dump(joint_detectors, 'joint_detectors.pkl')

    # Test on a few random images
    test_indices = random.sample(range(len(image_paths)), 5)

    for idx in test_indices:
        img_path = image_paths[idx]
        keypoints = keypoints_list[idx]

        print(f"\nProcessing image: {img_path}")

        # Detect all joints
        joint_positions = detect_all_joints(img_path, joint_detectors)

        # Apply anatomical constraints
        joint_positions = apply_anatomical_constraints(joint_positions)

        # Visualize skeleton
        vis_img = visualize_skeleton(img_path, joint_positions, ground_truth=keypoints)

        if vis_img is not None:
            # Save result
            output_path = f"skeleton_{os.path.basename(img_path)}"
            cv2.imwrite(output_path, vis_img)
            print(f"Skeleton visualization saved to: {output_path}")

            # Display result
            plt.figure(figsize=(12, 8))
            plt.imshow(cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB))
            plt.title(f"Human Pose Skeleton - {os.path.basename(img_path)}")
            plt.axis('off')
            plt.tight_layout()
            plt.savefig(f"figure_{os.path.basename(img_path)}.png")
            plt.show()

    print("Done!")

if __name__ == "__main__":
    main()