# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N_3kipNFW93TaVWNETQOEzfUxfTYCxff
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import tarfile
import zipfile
import hashlib

# Ensure matplotlib plots are shown in-line
# %matplotlib inline

# Dataset URLs provided
images_url = "https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz"
annots_url = "https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_u12_2.zip"

# Local file names
images_tar = "mpii_human_pose_v1.tar.gz"
annots_zip = "mpii_human_pose_v1_u12_2.zip"

# Directories for extracted content
images_dir = "./MPII/images/"
annots_dir = "./MPII/annotations/"

# Create extraction directories (if they do not exist)
os.makedirs(images_dir, exist_ok=True)
os.makedirs(annots_dir, exist_ok=True)

# Download the images tar.gz file if it doesn't exist
if not os.path.exists(images_tar):
    print("Downloading images archive...")
    !wget {images_url} -O {images_tar}
else:
    print("Images archive already exists.")

# Download the annotations zip file if it doesn't exist
if not os.path.exists(annots_zip):
    print("Downloading annotations archive...")
    !wget {annots_url} -O {annots_zip}
else:
    print("Annotations archive already exists.")

def compute_md5(fname, chunk_size=8192):
    md5 = hashlib.md5()
    with open(fname, 'rb') as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            md5.update(chunk)
    return md5.hexdigest()

print("MD5 for images archive:", compute_md5(images_tar))
print("MD5 for annotations archive:", compute_md5(annots_zip))

# Extract images archive
try:
    if not os.listdir(images_dir):
        print("Extracting images archive...")
        with tarfile.open(images_tar, "r:*") as tar:
            tar.extractall(path=images_dir)
        print("Images extracted successfully.")
    else:
        print("Images already extracted.")
except Exception as e:
    print("Error during images extraction:", e)

# Extract annotations archive
try:
    if not os.listdir(annots_dir):
        print("Extracting annotations archive...")
        with zipfile.ZipFile(annots_zip, "r") as zip_ref:
            zip_ref.extractall(annots_dir)
        print("Annotations extracted successfully.")
    else:
        print("Annotations already extracted.")
except Exception as e:
    print("Error during annotations extraction:", e)

# Recursively list all image files (assuming .jpg or .png) in the images directory
image_files = []
for root, dirs, files in os.walk(images_dir):
    for file in files:
        if file.lower().endswith(('.jpg', '.png')):
            image_files.append(os.path.join(root, file))

print("Number of images found:", len(image_files))

# Use the first image from the list (adjust selection as needed)
if image_files:
    sample_image_path = image_files[0]
    img = cv2.imread(sample_image_path)
    if img is None:
        raise Exception("Error loading image: " + sample_image_path)
    # Convert from BGR to RGB for proper visualization
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.figure(figsize=(8, 6))
    plt.imshow(img_rgb)
    plt.title("Original Image")
    plt.axis("off")
    plt.show()
else:
    raise Exception("No image files found!")

# Convert image to grayscale
gray_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)

# Apply Gaussian blur to reduce noise (kernel size may be tuned)
blurred_img = cv2.GaussianBlur(gray_img, (5, 5), 0)

# Display the grayscale and blurred images
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
ax1.imshow(gray_img, cmap="gray")
ax1.set_title("Grayscale Image")
ax1.axis("off")
ax2.imshow(blurred_img, cmap="gray")
ax2.set_title("Blurred Image")
ax2.axis("off")
plt.show()

# Set thresholds (tweak if necessary)
low_threshold = 50
high_threshold = 150
edges = cv2.Canny(blurred_img, low_threshold, high_threshold)

# Display the result of Canny edge detection
plt.figure(figsize=(8,6))
plt.imshow(edges, cmap="gray")
plt.title("Canny Edge Detection")
plt.axis("off")
plt.show()

# Find contours from the edges (using external contours)
contours, hierarchy = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Draw contours on a copy of the original image
contour_overlay = img_rgb.copy()
cv2.drawContours(contour_overlay, contours, -1, (0, 255, 0), 2)

plt.figure(figsize=(8,6))
plt.imshow(contour_overlay)
plt.title("Contours Overlay")
plt.axis("off")
plt.show()

print("Number of contours found:", len(contours))

# Convert grayscale image to float32 for the Harris detector
gray_float = np.float32(gray_img)

# Set parameters for Harris corner detection
block_size = 2
ksize = 3
k = 0.04
harris_response = cv2.cornerHarris(gray_float, block_size, ksize, k)

# Dilate the response for better visualization of corners
harris_response = cv2.dilate(harris_response, None)

# Threshold for identifying corners
corner_threshold = 0.01 * harris_response.max()
joint_img = img_rgb.copy()
joint_img[harris_response > corner_threshold] = [255, 0, 0]  # Mark detected corners in red

plt.figure(figsize=(8,6))
plt.imshow(joint_img)
plt.title("Harris Corners as Potential Joints")
plt.axis("off")
plt.show()

# Get coordinates of points where the Harris response exceeds the threshold
corner_points = np.argwhere(harris_response > corner_threshold)

def cluster_points(points, distance_threshold=15):
    """
    Cluster points that are within a specified distance threshold.
    Returns an array of cluster centroids.
    """
    points = list(points)
    clusters = []
    while points:
        base_point = points.pop(0)
        cluster = [base_point]
        to_remove = []
        for idx, other_point in enumerate(points):
            if np.linalg.norm(base_point - other_point) < distance_threshold:
                cluster.append(other_point)
                to_remove.append(idx)
        # Remove clustered points in reverse order so indices are valid
        for idx in sorted(to_remove, reverse=True):
            points.pop(idx)
        # The centroid of the cluster represents the joint candidate
        clusters.append(np.mean(cluster, axis=0).astype(int))
    return np.array(clusters)

clustered_joints = cluster_points(corner_points, distance_threshold=15)

# Overlay the clustered joint candidates on the original image
cluster_overlay = img_rgb.copy()
for (y, x) in clustered_joints:
    cv2.circle(cluster_overlay, (x, y), 5, (255, 0, 0), -1)

plt.figure(figsize=(8,6))
plt.imshow(cluster_overlay)
plt.title("Clustered Joint Candidates")
plt.axis("off")
plt.show()

print("Number of clustered joints:", len(clustered_joints))